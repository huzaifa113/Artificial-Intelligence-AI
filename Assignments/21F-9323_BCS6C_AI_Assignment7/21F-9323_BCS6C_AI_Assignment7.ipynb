{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAAD HABIB , ABDUL BASIT\n",
    "# 21F-9079   , 21F-9127\n",
    "# 6D , ASSI # 7\n",
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Activation Function:\n",
      "Iteration 1 weights: {'w1': array([0.10054374, 0.09932758, 0.09932758, 0.10054374]), 'w2': array([0.10054374, 0.09932758, 0.09932758, 0.10054374]), 'w3': array([0.10054374, 0.09932758, 0.09932758, 0.10054374]), 'w4': array([0.10054374, 0.09932758, 0.09932758, 0.10054374]), 'w5': array([0.1243424 , 0.06996739, 0.06996739, 0.1243424 ]), 'w6': array([0.1243424 , 0.06996739, 0.06996739, 0.1243424 ])}\n",
      "Iteration 2 weights: {'w1': array([0.10107591, 0.09867882, 0.09867882, 0.10107591]), 'w2': array([0.10107591, 0.09867882, 0.09867882, 0.10107591]), 'w3': array([0.10107591, 0.09867882, 0.09867882, 0.10107591]), 'w4': array([0.10107591, 0.09867882, 0.09867882, 0.10107591]), 'w5': array([0.14905314, 0.04020138, 0.04020138, 0.14905314]), 'w6': array([0.14905314, 0.04020138, 0.04020138, 0.14905314])}\n",
      "\n",
      "Tanh Activation Function:\n",
      "Iteration 1 weights: {'w1': array([0.11569576, 0.09728356, 0.09728356, 0.11569576]), 'w2': array([0.11569576, 0.09728356, 0.09728356, 0.11569576]), 'w3': array([0.11569576, 0.09728356, 0.09728356, 0.11569576]), 'w4': array([0.11569576, 0.09728356, 0.09728356, 0.11569576]), 'w5': array([0.16597701, 0.08899903, 0.08899903, 0.16597701]), 'w6': array([0.16597701, 0.08899903, 0.08899903, 0.16597701])}\n",
      "Iteration 2 weights: {'w1': array([0.12129608, 0.08729937, 0.08729937, 0.12129608]), 'w2': array([0.12129608, 0.08729937, 0.08729937, 0.12129608]), 'w3': array([0.12129608, 0.08729937, 0.08729937, 0.12129608]), 'w4': array([0.12129608, 0.08729937, 0.08729937, 0.12129608]), 'w5': array([0.19193018, 0.04505988, 0.04505988, 0.19193018]), 'w6': array([0.19193018, 0.04505988, 0.04505988, 0.19193018])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Tanh activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "# Input data\n",
    "X = np.array([[0, 0],\n",
    "              [1, 0],\n",
    "              [0, 1],\n",
    "              [1, 1]])\n",
    "\n",
    "# Target output\n",
    "Y = np.array([[1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [1]])\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights = {\n",
    "    'w1': 0.1, 'w2': 0.1, 'w3': 0.1, 'w4': 0.1,\n",
    "    'w5': 0.1, 'w6': 0.1\n",
    "}\n",
    "biases = {'b1': 0.1, 'b2': 0.1, 'b3': 0.1}\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# Function to perform a forward pass\n",
    "def forward(X, weights, biases, activation_func):\n",
    "    z1 = X[:, 0] * weights['w1'] + X[:, 1] * weights['w2'] + biases['b1']\n",
    "    z2 = X[:, 0] * weights['w3'] + X[:, 1] * weights['w4'] + biases['b2']\n",
    "    \n",
    "    a1 = activation_func(z1)\n",
    "    a2 = activation_func(z2)\n",
    "    \n",
    "    z3 = a1 * weights['w5'] + a2 * weights['w6'] + biases['b3']\n",
    "    yhat = activation_func(z3)\n",
    "    \n",
    "    return z1, z2, z3, a1, a2, yhat\n",
    "\n",
    "# Function to perform backpropagation\n",
    "def backpropagation(X, Y, weights, biases, activation_func, activation_derivative):\n",
    "    for i in range(2):  # Run for 2 iterations\n",
    "        # Forward pass\n",
    "        z1, z2, z3, a1, a2, yhat = forward(X, weights, biases, activation_func)\n",
    "        \n",
    "        # Calculate output error\n",
    "        output_error = Y - yhat\n",
    "        output_delta = output_error * activation_derivative(yhat)\n",
    "        \n",
    "        # Calculate hidden layer errors\n",
    "        hidden_error1 = output_delta * weights['w5']\n",
    "        hidden_error2 = output_delta * weights['w6']\n",
    "        \n",
    "        hidden_delta1 = hidden_error1 * activation_derivative(a1)\n",
    "        hidden_delta2 = hidden_error2 * activation_derivative(a2)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        weights['w5'] += lr * np.dot(a1, output_delta.T)\n",
    "        weights['w6'] += lr * np.dot(a2, output_delta.T)\n",
    "        \n",
    "        weights['w1'] += lr * np.dot(X[:, 0].T, hidden_delta1.T)\n",
    "        weights['w2'] += lr * np.dot(X[:, 1].T, hidden_delta1.T)\n",
    "        \n",
    "        weights['w3'] += lr * np.dot(X[:, 0].T, hidden_delta2.T)\n",
    "        weights['w4'] += lr * np.dot(X[:, 1].T, hidden_delta2.T)\n",
    "        \n",
    "        biases['b3'] += lr * np.sum(output_delta)\n",
    "        biases['b1'] += lr * np.sum(hidden_delta1)\n",
    "        biases['b2'] += lr * np.sum(hidden_delta2)\n",
    "        \n",
    "        print(f\"Iteration {i + 1} weights:\", weights)\n",
    "\n",
    "# Run for sigmoid activation function\n",
    "print(\"Sigmoid Activation Function:\")\n",
    "backpropagation(X, Y, weights, biases, sigmoid, sigmoid_derivative)\n",
    "\n",
    "# Reinitialize weights and biases for tanh\n",
    "weights = {\n",
    "    'w1': 0.1, 'w2': 0.1, 'w3': 0.1, 'w4': 0.1,\n",
    "    'w5': 0.1, 'w6': 0.1\n",
    "}\n",
    "biases = {'b1': 0.1, 'b2': 0.1, 'b3': 0.1}\n",
    "\n",
    "# Run for tanh activation function\n",
    "print(\"\\nTanh Activation Function:\")\n",
    "backpropagation(X, Y, weights, biases, tanh, tanh_derivative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "  Example 1:\n",
      "    X1 = 0, X2 = 0, Desired = 0, Actual = 0.5875\n",
      "    Delta_W1 = -0.0000, Delta_W2 = -0.0000, Delta_b = -0.0847\n",
      "    Updated W1 = 0.9719, Updated W2 = 0.3672, Updated b = 0.5892\n",
      "  Example 2:\n",
      "    X1 = 0, X2 = 1, Desired = 1, Actual = 0.7426\n",
      "    Delta_W1 = 0.0000, Delta_W2 = 0.0310, Delta_b = 0.0310\n",
      "    Updated W1 = 0.9719, Updated W2 = 0.3982, Updated b = 0.6202\n",
      "  Example 3:\n",
      "    X1 = 1, X2 = 0, Desired = 0, Actual = 0.9205\n",
      "    Delta_W1 = -0.0870, Delta_W2 = -0.0000, Delta_b = -0.0870\n",
      "    Updated W1 = 0.8849, Updated W2 = 0.3982, Updated b = 0.5331\n",
      "  Example 4:\n",
      "    X1 = 1, X2 = 1, Desired = 1, Actual = 0.9485\n",
      "    Delta_W1 = 0.0047, Delta_W2 = 0.0047, Delta_b = 0.0047\n",
      "    Updated W1 = 0.8895, Updated W2 = 0.4029, Updated b = 0.5378\n",
      "\n",
      "Iteration 2\n",
      "  Example 1:\n",
      "    X1 = 0, X2 = 0, Desired = 0, Actual = 0.4913\n",
      "    Delta_W1 = -0.0000, Delta_W2 = -0.0000, Delta_b = -0.0779\n",
      "    Updated W1 = 0.8895, Updated W2 = 0.4029, Updated b = 0.4599\n",
      "  Example 2:\n",
      "    X1 = 0, X2 = 1, Desired = 1, Actual = 0.6977\n",
      "    Delta_W1 = 0.0000, Delta_W2 = 0.0385, Delta_b = 0.0385\n",
      "    Updated W1 = 0.8895, Updated W2 = 0.4413, Updated b = 0.4984\n",
      "  Example 3:\n",
      "    X1 = 1, X2 = 0, Desired = 0, Actual = 0.8827\n",
      "    Delta_W1 = -0.0881, Delta_W2 = -0.0000, Delta_b = -0.0881\n",
      "    Updated W1 = 0.8014, Updated W2 = 0.4413, Updated b = 0.4103\n",
      "  Example 4:\n",
      "    X1 = 1, X2 = 1, Desired = 1, Actual = 0.9293\n",
      "    Delta_W1 = 0.0066, Delta_W2 = 0.0066, Delta_b = 0.0066\n",
      "    Updated W1 = 0.8080, Updated W2 = 0.4479, Updated b = 0.4169\n",
      "\n",
      "Iteration 3\n",
      "  Example 1:\n",
      "    X1 = 0, X2 = 0, Desired = 0, Actual = 0.3943\n",
      "    Delta_W1 = -0.0000, Delta_W2 = -0.0000, Delta_b = -0.0678\n",
      "    Updated W1 = 0.8080, Updated W2 = 0.4479, Updated b = 0.3491\n",
      "  Example 2:\n",
      "    X1 = 0, X2 = 1, Desired = 1, Actual = 0.6624\n",
      "    Delta_W1 = 0.0000, Delta_W2 = 0.0448, Delta_b = 0.0448\n",
      "    Updated W1 = 0.8080, Updated W2 = 0.4928, Updated b = 0.3939\n",
      "  Example 3:\n",
      "    X1 = 1, X2 = 0, Desired = 0, Actual = 0.8343\n",
      "    Delta_W1 = -0.0891, Delta_W2 = -0.0000, Delta_b = -0.0891\n",
      "    Updated W1 = 0.7190, Updated W2 = 0.4928, Updated b = 0.3049\n",
      "  Example 4:\n",
      "    X1 = 1, X2 = 1, Desired = 1, Actual = 0.9081\n",
      "    Delta_W1 = 0.0088, Delta_W2 = 0.0088, Delta_b = 0.0088\n",
      "    Updated W1 = 0.7278, Updated W2 = 0.5016, Updated b = 0.3137\n",
      "\n",
      "Final weights: W1 = 0.7278, W2 = 0.5016, b = 0.3137\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Inputs and desired outputs\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "D = np.array([0, 1, 0, 1])\n",
    "\n",
    "# Initialize weights and bias\n",
    "W1, W2, b = np.random.rand(3)\n",
    "\n",
    "# Learning parameters\n",
    "eta = 0.2\n",
    "iterations = 3\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# Training the ANN\n",
    "for iteration in range(iterations):\n",
    "    print(f\"Iteration {iteration + 1}\")\n",
    "    for i in range(len(X)):\n",
    "        X1, X2 = X[i]\n",
    "        desired_output = D[i]\n",
    "        \n",
    "        # Forward pass\n",
    "        actual_output = W1 * X1 + W2 * X2 + b\n",
    "        y = tanh(actual_output)\n",
    "        \n",
    "        # Calculate error\n",
    "        error = desired_output - y\n",
    "        \n",
    "        # Backpropagation\n",
    "        delta = error * tanh_derivative(y)\n",
    "        \n",
    "        # Weight updates\n",
    "        delta_W1 = eta * delta * X1\n",
    "        delta_W2 = eta * delta * X2\n",
    "        delta_b = eta * delta\n",
    "        \n",
    "        W1 += delta_W1\n",
    "        W2 += delta_W2\n",
    "        b += delta_b\n",
    "        \n",
    "        print(f\"  Example {i + 1}:\")\n",
    "        print(f\"    X1 = {X1}, X2 = {X2}, Desired = {desired_output}, Actual = {y:.4f}\")\n",
    "        print(f\"    Delta_W1 = {delta_W1:.4f}, Delta_W2 = {delta_W2:.4f}, Delta_b = {delta_b:.4f}\")\n",
    "        print(f\"    Updated W1 = {W1:.4f}, Updated W2 = {W2:.4f}, Updated b = {b:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Final weights after training\n",
    "print(f\"Final weights: W1 = {W1:.4f}, W2 = {W2:.4f}, b = {b:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Accuracy (test set): 1.00\n",
      "Decision Tree Classifier Cross-Validation Accuracy: 0.95 (+/- 0.03)\n",
      "Decision Tree Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        19\n",
      "Iris-versicolor       1.00      1.00      1.00        13\n",
      " Iris-virginica       1.00      1.00      1.00        13\n",
      "\n",
      "       accuracy                           1.00        45\n",
      "      macro avg       1.00      1.00      1.00        45\n",
      "   weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Random Forest Classifier Accuracy (test set): 1.00\n",
      "Random Forest Classifier Cross-Validation Accuracy: 0.97 (+/- 0.02)\n",
      "Random Forest Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        19\n",
      "Iris-versicolor       1.00      1.00      1.00        13\n",
      " Iris-virginica       1.00      1.00      1.00        13\n",
      "\n",
      "       accuracy                           1.00        45\n",
      "      macro avg       1.00      1.00      1.00        45\n",
      "   weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Improvement in Accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "iris_data = pd.read_csv('IRIS.csv')\n",
    "\n",
    "# Extract features and labels\n",
    "X = iris_data.drop('species', axis=1)\n",
    "y = iris_data['species']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the data: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build and train a Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the Decision Tree classifier\n",
    "y_pred_dt = dt_classifier.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Cross-validation for Decision Tree\n",
    "cv_scores_dt = cross_val_score(dt_classifier, X, y, cv=5)\n",
    "print(f'Decision Tree Classifier Accuracy (test set): {accuracy_dt:.2f}')\n",
    "print(f'Decision Tree Classifier Cross-Validation Accuracy: {cv_scores_dt.mean():.2f} (+/- {cv_scores_dt.std():.2f})')\n",
    "\n",
    "# Detailed classification report for Decision Tree\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# Build and train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the Random Forest classifier\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Cross-validation for Random Forest\n",
    "cv_scores_rf = cross_val_score(rf_classifier, X, y, cv=5)\n",
    "print(f'Random Forest Classifier Accuracy (test set): {accuracy_rf:.2f}')\n",
    "print(f'Random Forest Classifier Cross-Validation Accuracy: {cv_scores_rf.mean():.2f} (+/- {cv_scores_rf.std():.2f})')\n",
    "\n",
    "# Detailed classification report for Random Forest\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Compare the performance\n",
    "performance_improvement = accuracy_rf - accuracy_dt\n",
    "print(f'Improvement in Accuracy: {performance_improvement:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
