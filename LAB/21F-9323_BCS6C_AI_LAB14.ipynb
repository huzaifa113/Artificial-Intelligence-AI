{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**TASK # 1**"
      ],
      "metadata": {
        "id": "FFlZjhFA8N57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimplePerceptron:\n",
        "    def __init__(self, input_size, threshold=100, learning_rate=0.5):\n",
        "        self.threshold = threshold\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights with random values\n",
        "        self.weights = np.random.rand(input_size + 1)  # Include bias\n",
        "\n",
        "    def train(self, training_inputs, labels, bias):\n",
        "        for epoch in range(self.threshold):\n",
        "            for inputs, label in zip(training_inputs, labels):\n",
        "                # Forward pass\n",
        "                summation = np.dot(inputs, self.weights[1:]) + self.weights[0] + bias\n",
        "\n",
        "                # Apply step function\n",
        "                activation = 1 if summation > 0 else 0\n",
        "\n",
        "                # Update weights\n",
        "                self.weights[1:] += self.learning_rate * (label - activation) * inputs\n",
        "                self.weights[0] += self.learning_rate * (label - activation)\n",
        "\n",
        "                # Print weights after each epoch\n",
        "                print(\"Epoch {}, Bias {}: Weights: {}\".format(epoch, bias, self.weights))\n",
        "\n",
        "    def predict(self, inputs, bias):\n",
        "        # Forward pass\n",
        "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0] + bias\n",
        "\n",
        "        # Apply step function\n",
        "        activation = 1 if summation > 0 else 0\n",
        "\n",
        "        return activation\n",
        "\n",
        "def main():\n",
        "    # Input dataset for logical OR gate\n",
        "    training_inputs = np.array([[0, 0],\n",
        "                                [0, 1],\n",
        "                                [1, 0],\n",
        "                                [1, 1]])\n",
        "\n",
        "    # Output labels for logical OR gate\n",
        "    labels = np.array([0, 1, 1, 1])\n",
        "\n",
        "    # Instantiate SimplePerceptron\n",
        "    perceptron = SimplePerceptron(input_size=2)\n",
        "\n",
        "    # Bias values to iterate over\n",
        "    bias_values = [0.1, 0.5, 0.7]\n",
        "\n",
        "    # Train and predict for each bias value\n",
        "    for bias in bias_values:\n",
        "        print(\"\\nTraining and Predictions for Bias:\", bias)\n",
        "        perceptron.train(training_inputs, labels, bias)\n",
        "\n",
        "        # Make predictions\n",
        "        for inputs in training_inputs:\n",
        "            output = perceptron.predict(inputs, bias)\n",
        "            print(\"Input:\", inputs, \"Predicted Output:\", output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_apYnQ5oxku",
        "outputId": "e8c5d2da-be55-4c77-abe6-750ca3c463b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and Predictions for Bias: 0.1\n",
            "Epoch 0, Bias 0.1: Weights: [0.38410713 0.40547581 0.99466006]\n",
            "Epoch 0, Bias 0.1: Weights: [0.38410713 0.40547581 0.99466006]\n",
            "Epoch 0, Bias 0.1: Weights: [0.38410713 0.40547581 0.99466006]\n",
            "Epoch 0, Bias 0.1: Weights: [0.38410713 0.40547581 0.99466006]\n",
            "Epoch 1, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 1, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 1, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 1, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 2, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 2, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 2, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 2, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 3, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 3, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 3, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 3, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 4, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 4, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 4, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 4, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 5, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 5, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 5, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 5, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 6, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 6, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 6, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 6, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 7, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 7, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 7, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 7, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 8, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 8, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 8, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 8, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 9, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 9, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 9, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 9, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 10, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 10, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 10, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 10, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 11, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 11, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 11, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 11, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 12, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 12, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 12, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 12, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 13, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 13, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 13, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 13, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 14, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 14, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 14, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 14, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 15, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 15, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 15, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 15, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 16, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 16, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 16, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 16, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 17, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 17, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 17, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 17, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 18, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 18, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 18, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 18, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 19, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 19, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 19, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 19, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 20, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 20, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 20, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 20, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 21, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 21, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 21, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 21, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 22, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 22, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 22, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 22, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 23, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 23, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 23, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 23, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 24, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 24, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 24, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 24, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 25, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 25, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 25, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 25, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 26, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 26, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 26, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 26, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 27, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 27, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 27, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 27, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 28, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 28, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 28, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 28, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 29, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 29, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 29, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 29, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 30, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 30, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 30, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 30, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 31, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 31, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 31, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 31, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 32, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 32, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 32, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 32, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 33, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 33, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 33, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 33, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 34, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 34, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 34, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 34, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 35, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 35, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 35, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 35, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 36, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 36, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 36, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 36, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 37, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 37, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 37, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 37, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 38, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 38, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 38, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 38, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 39, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 39, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 39, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 39, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 40, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 40, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 40, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 40, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 41, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 41, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 41, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 41, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 42, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 42, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 42, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 42, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 43, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 43, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 43, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 43, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 44, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 44, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 44, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 44, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 45, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 45, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 45, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 45, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 46, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 46, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 46, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 46, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 47, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 47, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 47, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 47, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 48, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 48, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 48, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 48, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 49, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 49, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 49, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 49, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 50, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 50, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 50, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 50, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 51, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 51, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 51, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 51, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 52, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 52, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 52, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 52, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 53, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 53, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 53, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 53, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 54, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 54, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 54, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 54, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 55, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 55, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 55, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 55, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 56, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 56, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 56, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 56, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 57, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 57, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 57, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 57, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 58, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 58, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 58, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 58, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 59, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 59, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 59, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 59, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 60, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 60, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 60, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 60, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 61, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 61, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 61, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 61, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 62, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 62, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 62, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 62, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 63, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 63, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 63, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 63, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 64, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 64, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 64, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 64, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 65, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 65, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 65, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 65, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 66, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 66, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 66, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 66, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 67, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 67, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 67, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 67, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 68, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 68, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 68, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 68, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 69, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 69, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 69, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 69, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 70, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 70, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 70, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 70, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 71, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 71, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 71, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 71, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 72, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 72, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 72, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 72, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 73, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 73, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 73, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 73, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 74, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 74, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 74, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 74, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 75, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 75, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 75, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 75, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 76, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 76, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 76, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 76, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 77, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 77, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 77, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 77, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 78, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 78, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 78, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 78, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 79, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 79, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 79, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 79, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 80, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 80, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 80, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 80, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 81, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 81, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 81, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 81, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 82, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 82, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 82, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 82, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 83, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 83, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 83, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 83, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 84, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 84, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 84, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 84, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 85, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 85, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 85, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 85, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 86, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 86, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 86, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 86, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 87, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 87, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 87, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 87, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 88, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 88, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 88, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 88, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 89, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 89, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 89, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 89, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 90, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 90, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 90, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 90, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 91, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 91, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 91, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 91, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 92, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 92, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 92, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 92, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 93, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 93, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 93, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 93, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 94, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 94, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 94, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 94, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 95, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 95, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 95, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 95, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 96, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 96, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 96, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 96, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 97, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 97, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 97, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 97, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 98, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 98, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 98, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 98, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 99, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 99, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 99, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Epoch 99, Bias 0.1: Weights: [-0.11589287  0.40547581  0.99466006]\n",
            "Input: [0 0] Predicted Output: 0\n",
            "Input: [0 1] Predicted Output: 1\n",
            "Input: [1 0] Predicted Output: 1\n",
            "Input: [1 1] Predicted Output: 1\n",
            "\n",
            "Training and Predictions for Bias: 0.5\n",
            "Epoch 0, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 0, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 0, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 0, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 1, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 1, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 1, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 1, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 2, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 2, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 2, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 2, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 3, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 3, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 3, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 3, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 4, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 4, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 4, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 4, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 5, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 5, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 5, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 5, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 6, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 6, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 6, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 6, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 7, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 7, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 7, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 7, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 8, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 8, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 8, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 8, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 9, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 9, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 9, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 9, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 10, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 10, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 10, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 10, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 11, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 11, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 11, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 11, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 12, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 12, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 12, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 12, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 13, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 13, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 13, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 13, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 14, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 14, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 14, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 14, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 15, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 15, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 15, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 15, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 16, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 16, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 16, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 16, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 17, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 17, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 17, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 17, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 18, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 18, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 18, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 18, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 19, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 19, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 19, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 19, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 20, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 20, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 20, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 20, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 21, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 21, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 21, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 21, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 22, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 22, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 22, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 22, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 23, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 23, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 23, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 23, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 24, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 24, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 24, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 24, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 25, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 25, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 25, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 25, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 26, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 26, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 26, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 26, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 27, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 27, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 27, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 27, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 28, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 28, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 28, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 28, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 29, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 29, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 29, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 29, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 30, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 30, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 30, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 30, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 31, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 31, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 31, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 31, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 32, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 32, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 32, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 32, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 33, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 33, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 33, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 33, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 34, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 34, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 34, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 34, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 35, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 35, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 35, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 35, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 36, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 36, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 36, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 36, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 37, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 37, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 37, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 37, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 38, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 38, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 38, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 38, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 39, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 39, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 39, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 39, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 40, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 40, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 40, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 40, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 41, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 41, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 41, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 41, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 42, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 42, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 42, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 42, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 43, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 43, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 43, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 43, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 44, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 44, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 44, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 44, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 45, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 45, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 45, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 45, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 46, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 46, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 46, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 46, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 47, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 47, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 47, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 47, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 48, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 48, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 48, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 48, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 49, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 49, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 49, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 49, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 50, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 50, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 50, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 50, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 51, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 51, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 51, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 51, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 52, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 52, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 52, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 52, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 53, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 53, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 53, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 53, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 54, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 54, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 54, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 54, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 55, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 55, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 55, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 55, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 56, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 56, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 56, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 56, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 57, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 57, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 57, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 57, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 58, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 58, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 58, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 58, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 59, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 59, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 59, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 59, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 60, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 60, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 60, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 60, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 61, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 61, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 61, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 61, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 62, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 62, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 62, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 62, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 63, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 63, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 63, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 63, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 64, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 64, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 64, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 64, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 65, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 65, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 65, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 65, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 66, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 66, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 66, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 66, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 67, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 67, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 67, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 67, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 68, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 68, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 68, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 68, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 69, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 69, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 69, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 69, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 70, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 70, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 70, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 70, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 71, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 71, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 71, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 71, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 72, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 72, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 72, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 72, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 73, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 73, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 73, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 73, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 74, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 74, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 74, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 74, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 75, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 75, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 75, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 75, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 76, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 76, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 76, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 76, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 77, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 77, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 77, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 77, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 78, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 78, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 78, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 78, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 79, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 79, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 79, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 79, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 80, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 80, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 80, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 80, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 81, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 81, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 81, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 81, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 82, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 82, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 82, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 82, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 83, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 83, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 83, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 83, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 84, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 84, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 84, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 84, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 85, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 85, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 85, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 85, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 86, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 86, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 86, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 86, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 87, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 87, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 87, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 87, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 88, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 88, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 88, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 88, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 89, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 89, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 89, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 89, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 90, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 90, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 90, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 90, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 91, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 91, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 91, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 91, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 92, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 92, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 92, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 92, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 93, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 93, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 93, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 93, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 94, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 94, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 94, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 94, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 95, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 95, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 95, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 95, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 96, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 96, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 96, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 96, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 97, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 97, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 97, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 97, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 98, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 98, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 98, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 98, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 99, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 99, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 99, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Epoch 99, Bias 0.5: Weights: [-0.61589287  0.40547581  0.99466006]\n",
            "Input: [0 0] Predicted Output: 0\n",
            "Input: [0 1] Predicted Output: 1\n",
            "Input: [1 0] Predicted Output: 1\n",
            "Input: [1 1] Predicted Output: 1\n",
            "\n",
            "Training and Predictions for Bias: 0.7\n",
            "Epoch 0, Bias 0.7: Weights: [-1.11589287  0.40547581  0.99466006]\n",
            "Epoch 0, Bias 0.7: Weights: [-1.11589287  0.40547581  0.99466006]\n",
            "Epoch 0, Bias 0.7: Weights: [-0.61589287  0.90547581  0.99466006]\n",
            "Epoch 0, Bias 0.7: Weights: [-0.61589287  0.90547581  0.99466006]\n",
            "Epoch 1, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 1, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 1, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 1, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 2, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 2, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 2, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 2, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 3, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 3, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 3, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 3, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 4, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 4, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 4, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 4, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 5, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 5, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 5, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 5, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 6, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 6, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 6, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 6, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 7, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 7, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 7, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 7, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 8, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 8, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 8, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 8, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 9, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 9, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 9, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 9, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 10, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 10, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 10, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 10, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 11, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 11, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 11, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 11, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 12, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 12, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 12, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 12, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 13, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 13, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 13, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 13, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 14, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 14, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 14, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 14, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 15, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 15, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 15, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 15, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 16, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 16, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 16, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 16, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 17, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 17, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 17, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 17, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 18, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 18, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 18, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 18, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 19, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 19, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 19, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 19, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 20, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 20, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 20, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 20, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 21, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 21, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 21, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 21, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 22, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 22, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 22, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 22, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 23, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 23, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 23, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 23, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 24, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 24, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 24, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 24, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 25, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 25, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 25, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 25, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 26, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 26, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 26, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 26, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 27, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 27, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 27, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 27, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 28, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 28, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 28, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 28, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 29, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 29, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 29, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 29, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 30, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 30, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 30, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 30, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 31, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 31, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 31, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 31, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 32, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 32, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 32, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 32, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 33, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 33, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 33, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 33, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 34, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 34, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 34, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 34, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 35, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 35, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 35, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 35, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 36, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 36, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 36, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 36, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 37, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 37, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 37, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 37, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 38, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 38, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 38, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 38, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 39, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 39, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 39, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 39, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 40, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 40, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 40, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 40, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 41, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 41, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 41, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 41, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 42, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 42, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 42, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 42, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 43, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 43, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 43, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 43, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 44, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 44, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 44, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 44, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 45, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 45, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 45, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 45, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 46, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 46, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 46, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 46, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 47, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 47, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 47, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 47, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 48, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 48, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 48, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 48, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 49, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 49, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 49, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 49, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 50, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 50, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 50, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 50, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 51, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 51, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 51, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 51, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 52, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 52, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 52, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 52, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 53, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 53, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 53, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 53, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 54, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 54, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 54, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 54, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 55, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 55, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 55, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 55, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 56, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 56, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 56, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 56, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 57, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 57, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 57, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 57, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 58, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 58, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 58, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 58, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 59, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 59, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 59, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 59, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 60, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 60, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 60, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 60, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 61, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 61, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 61, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 61, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 62, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 62, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 62, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 62, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 63, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 63, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 63, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 63, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 64, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 64, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 64, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 64, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 65, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 65, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 65, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 65, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 66, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 66, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 66, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 66, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 67, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 67, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 67, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 67, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 68, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 68, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 68, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 68, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 69, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 69, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 69, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 69, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 70, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 70, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 70, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 70, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 71, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 71, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 71, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 71, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 72, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 72, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 72, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 72, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 73, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 73, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 73, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 73, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 74, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 74, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 74, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 74, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 75, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 75, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 75, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 75, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 76, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 76, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 76, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 76, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 77, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 77, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 77, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 77, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 78, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 78, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 78, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 78, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 79, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 79, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 79, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 79, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 80, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 80, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 80, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 80, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 81, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 81, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 81, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 81, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 82, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 82, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 82, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 82, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 83, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 83, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 83, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 83, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 84, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 84, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 84, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 84, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 85, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 85, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 85, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 85, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 86, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 86, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 86, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 86, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 87, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 87, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 87, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 87, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 88, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 88, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 88, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 88, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 89, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 89, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 89, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 89, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 90, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 90, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 90, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 90, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 91, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 91, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 91, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 91, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 92, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 92, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 92, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 92, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 93, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 93, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 93, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 93, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 94, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 94, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 94, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 94, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 95, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 95, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 95, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 95, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 96, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 96, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 96, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 96, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 97, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 97, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 97, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 97, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 98, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 98, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 98, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 98, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 99, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 99, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 99, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Epoch 99, Bias 0.7: Weights: [-1.11589287  0.90547581  0.99466006]\n",
            "Input: [0 0] Predicted Output: 0\n",
            "Input: [0 1] Predicted Output: 1\n",
            "Input: [1 0] Predicted Output: 1\n",
            "Input: [1 1] Predicted Output: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK # 2**"
      ],
      "metadata": {
        "id": "9-c-kNah8JPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleMLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size, threshold=10000, learning_rate=0.1):\n",
        "        self.threshold = threshold\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights with Xavier initialization\n",
        "        self.weights_input_hidden = np.random.randn(input_size + 1, hidden_size) * np.sqrt(2 / (input_size + hidden_size))\n",
        "        self.weights_hidden_output = np.random.randn(hidden_size + 1, output_size) * np.sqrt(2 / (hidden_size + output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def train(self, training_inputs, labels):\n",
        "        # Training loop\n",
        "        for _ in range(self.threshold):\n",
        "            for inputs, label in zip(training_inputs, labels):\n",
        "                # Forward pass\n",
        "                inputs_with_bias = np.append(inputs, 1)  # Add bias term\n",
        "                hidden_inputs = np.dot(inputs_with_bias, self.weights_input_hidden)\n",
        "                hidden_outputs = self.sigmoid(hidden_inputs)\n",
        "\n",
        "                hidden_outputs_with_bias = np.append(hidden_outputs, 1)  # Add bias term\n",
        "                final_inputs = np.dot(hidden_outputs_with_bias, self.weights_hidden_output)\n",
        "                final_outputs = self.sigmoid(final_inputs)\n",
        "\n",
        "                # Backpropagation\n",
        "                output_error = label - final_outputs\n",
        "                output_delta = output_error * self.sigmoid_derivative(final_outputs)\n",
        "\n",
        "                hidden_error = np.dot(output_delta, self.weights_hidden_output[:-1].T)\n",
        "                hidden_delta = hidden_error * self.sigmoid_derivative(hidden_outputs)\n",
        "\n",
        "                # Update weights\n",
        "                self.weights_hidden_output += self.learning_rate * np.outer(hidden_outputs_with_bias, output_delta)\n",
        "                self.weights_input_hidden += self.learning_rate * np.outer(inputs_with_bias, hidden_delta)\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        # Forward pass for prediction\n",
        "        inputs_with_bias = np.append(inputs, 1)  # Add bias term\n",
        "        hidden_inputs = np.dot(inputs_with_bias, self.weights_input_hidden)\n",
        "        hidden_outputs = self.sigmoid(hidden_inputs)\n",
        "\n",
        "        hidden_outputs_with_bias = np.append(hidden_outputs, 1)  # Add bias term\n",
        "        final_inputs = np.dot(hidden_outputs_with_bias, self.weights_hidden_output)\n",
        "        final_outputs = self.sigmoid(final_inputs)\n",
        "\n",
        "        return final_outputs\n",
        "\n",
        "def main():\n",
        "    # XOR gate dataset\n",
        "    training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    labels = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    # Create and train MLP\n",
        "    mlp = SimpleMLP(input_size=2, hidden_size=8, output_size=1, threshold=10000, learning_rate=0.1)\n",
        "    mlp.train(training_inputs, labels)\n",
        "\n",
        "    # Predictions\n",
        "    for inputs, label in zip(training_inputs, labels):\n",
        "        predicted_output = mlp.predict(inputs)[0]\n",
        "        print(\"Input:\", inputs, \"Actual Output:\", label[0], \"Predicted Output:\", round(predicted_output))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUqz_gexqsdG",
        "outputId": "591be063-af85-4b4d-a5b8-970124ec7247"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0] Actual Output: 0 Predicted Output: 0\n",
            "Input: [0 1] Actual Output: 1 Predicted Output: 1\n",
            "Input: [1 0] Actual Output: 1 Predicted Output: 1\n",
            "Input: [1 1] Actual Output: 0 Predicted Output: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task # 3**"
      ],
      "metadata": {
        "id": "toWXpFOE8YEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "import random\n",
        "\n",
        "# Load MNIST data\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Data preprocessing\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Model architecture\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model training\n",
        "history = model.fit(train_images, train_labels, epochs=5, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Model evaluation\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n",
        "# Predict on random images\n",
        "num_examples = 5\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(num_examples):\n",
        "    random_index = random.randint(0, len(test_images) - 1)\n",
        "    random_image = test_images[random_index]\n",
        "    random_label = test_labels[random_index]\n",
        "    prediction = model.predict(np.expand_dims(random_image, axis=0))\n",
        "\n",
        "    plt.subplot(1, num_examples, i + 1)\n",
        "    plt.imshow(random_image, cmap='gray')\n",
        "    plt.title(f\"Prediction: {np.argmax(prediction)}, True Label: {random_label}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "vB7LeRue8F_c",
        "outputId": "eea8563e-501b-4302-fd77-f01f5ed90dc7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.2775 - accuracy: 0.9216 - val_loss: 0.1275 - val_accuracy: 0.9660\n",
            "Epoch 2/5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.1220 - accuracy: 0.9637 - val_loss: 0.0968 - val_accuracy: 0.9732\n",
            "Epoch 3/5\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.0834 - accuracy: 0.9749 - val_loss: 0.0830 - val_accuracy: 0.9765\n",
            "Epoch 4/5\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0616 - accuracy: 0.9811 - val_loss: 0.0757 - val_accuracy: 0.9790\n",
            "Epoch 5/5\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.0482 - accuracy: 0.9847 - val_loss: 0.0752 - val_accuracy: 0.9792\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0755 - accuracy: 0.9767\n",
            "Test accuracy: 0.9767000079154968\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}